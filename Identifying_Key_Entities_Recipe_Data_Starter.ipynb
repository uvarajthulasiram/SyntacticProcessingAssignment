{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-ksMVNgeyiLN",
        "1y18LwoqyFpk",
        "bhNG_XC1r4Qw",
        "-JtvsBYur-oV",
        "dpJQu3JE_P7Z",
        "JbriClEV9CW5",
        "Qtqtij2-CD2m",
        "_RJEStPSC9PB",
        "qJdYJ2TEDBzd",
        "hJm2nUw0998s"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Identifying Key Entities in Recipe Data**"
      ],
      "metadata": {
        "id": "42UBKEnat_xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Business Objective**:\n",
        "The goal of this assignment is to train a Named Entity Recognition (NER) model using Conditional Random Fields (CRF) to extract key entities from recipe data. The model will classify words into predefined categories such as ingredients, quantities and units, enabling the creation of a structured database of recipes and ingredients that can be used to power advanced features in recipe management systems, dietary tracking apps, or e-commerce platforms."
      ],
      "metadata": {
        "id": "Pme3h_fduOKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Description**\n",
        "The given data is in JSON format, representing a **structured recipe ingredient list** with **Named Entity Recognition (NER) labels**. Below is a breakdown of the data fields:\n",
        "\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"input\": \"6 Karela Bitter Gourd Pavakkai Salt 1 Onion 3 tablespoon Gram flour besan 2 teaspoons Turmeric powder Haldi Red Chilli Cumin seeds Jeera Coriander Powder Dhania Amchur Dry Mango Sunflower Oil\",\n",
        "        \"pos\": \"quantity ingredient ingredient ingredient ingredient ingredient quantity ingredient quantity unit ingredient ingredient ingredient quantity unit ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": \"2-1/2 cups rice cooked 3 tomatoes teaspoons BC Belle Bhat powder 1 teaspoon chickpea lentils 1/2 cumin seeds white urad dal mustard green chilli dry red 2 cashew or peanuts 1-1/2 tablespoon oil asafoetida\",\n",
        "      \"pos\": \"quantity unit ingredient ingredient quantity ingredient unit ingredient ingredient ingredient ingredient quantity unit ingredient ingredient quantity ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient quantity ingredient ingredient ingredient quantity unit ingredient ingredient\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "FXzoAs8evNG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Key**  | **Description**  |\n",
        "|----------|-----------------|\n",
        "| `input`  | Contains a raw ingredient list from a recipe. |\n",
        "| `pos`    | Represents the corresponding part-of-speech (POS) tags or NER labels, identifying quantities, ingredients, and units. |\n"
      ],
      "metadata": {
        "id": "LSDcNvJlwC6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1** Import libraries"
      ],
      "metadata": {
        "id": "phenosA4se1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.1** Installation of sklearn-crfsuite"
      ],
      "metadata": {
        "id": "Br-jQHin3kQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn-crfsuite is a Python wrapper for CRFsuite, a fast and efficient implementation of Conditional Random Fields (CRFs). It is designed to integrate seamlessly with scikit-learn for structured prediction tasks such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and chunking."
      ],
      "metadata": {
        "id": "DPhaJSfCwpfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installation of sklearn_crfsuite\n",
        "!pip install sklearn_crfsuite==0.5.0"
      ],
      "metadata": {
        "id": "_QawokgQXAMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2** Import necessary libraries"
      ],
      "metadata": {
        "id": "svqZwrHT3rzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "v2zLbaB0w1ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hlp-Ln4WsaV"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import json  # For handling JSON data\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import re  # For regular expressions (useful for text preprocessing)\n",
        "import matplotlib.pyplot as plt  # For visualisation\n",
        "import seaborn as sns  # For advanced data visualisation\n",
        "import sklearn_crfsuite  # CRF (Conditional Random Fields) implementation for sequence modeling\n",
        "import numpy as np  # For numerical computations\n",
        "# Saving and loading machine learning models\n",
        "import joblib\n",
        "import random\n",
        "import spacy\n",
        "from IPython.display import display, Markdown # For displaying well-formatted output\n",
        "\n",
        "from fractions import Fraction  # For handling fractional values in numerical data\n",
        "# Importing tools for feature engineering and model training\n",
        "from collections import Counter  # For counting occurrences of elements in a list\n",
        "from sklearn.model_selection import train_test_split  # For splitting dataset into train and test sets\n",
        "from sklearn_crfsuite import metrics  # For evaluating CRF models\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure pandas displays full content\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.expand_frame_repr', False)"
      ],
      "metadata": {
        "id": "k3_LR6N_2cli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2** Data Ingestion and Preparation <font color = red>[25 marks]</font> <br>"
      ],
      "metadata": {
        "id": "fUOu_u0fyMfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1** *Read Recipe Data from Dataframe and prepare the data for analysis* <font color = red>[12 marks]</font> <br>\n",
        "Read the data from JSON file, print first five rows and describe the dataframe"
      ],
      "metadata": {
        "id": "-ksMVNgeyiLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.1.1** **Define a *load_json_dataframe* function** <font color = red>[7 marks]</font> <br>\n",
        "\n",
        "Define a function that takes path of the ingredient_and_quantity.json file and reads it, convert it into dataframe - df and return it."
      ],
      "metadata": {
        "id": "kxn28jL3z4GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to load json file to a dataframe\n"
      ],
      "metadata": {
        "id": "dq6UgUYcPyOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.1.2** **Execute the *load_json_dataframe* function** <font color = red>[2 marks]</font> <br>"
      ],
      "metadata": {
        "id": "1NlhkH_605IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the json file by giving the file path and create a dataframe\n"
      ],
      "metadata": {
        "id": "UONMkMsrxdxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.1.3** **Describe the dataframe** <font color = red>[3 marks]</font> <br>\n",
        "\n",
        "Print first five rows of dataframe along with dimensions. Display the information of dataframe"
      ],
      "metadata": {
        "id": "_1VkDbev3UHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display first five rows of the dataframe - df\n"
      ],
      "metadata": {
        "id": "yZFj2skZxgpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the dimensions of dataframe - df\n"
      ],
      "metadata": {
        "id": "Y7cA28XSx1I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the information of the dataframe\n"
      ],
      "metadata": {
        "id": "Q-gsbEhJx2rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2** *Recipe Data Manipulation* <font color = red>[13 marks]</font> <br>\n",
        "Create derived metrics in dataframe and provide insights of the dataframe"
      ],
      "metadata": {
        "id": "1y18LwoqyFpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2.1** **Create input_tokens and pos_tokens columns by splitting the input and pos from the dataframe** <font color = red>[3 marks]</font> <br>\n",
        "Split the input and pos into input_tokens and pos_tokens in the dataframe and display it in the dataframe"
      ],
      "metadata": {
        "id": "bhNG_XC1r4Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the input and pos into input_tokens and pos_tokens in the dataframe\n",
        "\n",
        "# Tokenize input\n",
        "# Tokenize POS"
      ],
      "metadata": {
        "id": "nma6uJwmXUas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display first five rows of the dataframe - df\n"
      ],
      "metadata": {
        "id": "9g-ajvFBzaaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2.2** **Provide the length for input_tokens and pos_tokens and validate their length** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create input_length and pos_length columns in the dataframe and validate both the lengths. Check for the rows that are unequal in input and pos length\n"
      ],
      "metadata": {
        "id": "-JtvsBYur-oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create input_length and pos_length columns for the input_tokens and pos-tokens\n"
      ],
      "metadata": {
        "id": "GeVRD2IK1Jrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for the equality of input_length and pos_length in the dataframe\n"
      ],
      "metadata": {
        "id": "BPMOlLnz1P1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2.3** **Define a unique_labels function and validate the labels in pos_tokens** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a unique_labels function which checks for all the unique pos labels in the recipe & execute it.\n"
      ],
      "metadata": {
        "id": "dpJQu3JE_P7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a unique_labels function to checks for all the unique pos labels in the recipe & print it\n"
      ],
      "metadata": {
        "id": "-4aMFCxXO_GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2.3** **Provide the insights seen in the recipe data after validation** <font color = red>[1 marks]</font> <br>\n",
        "\n",
        "Provide the indexes that requires cleaning and formatting in the dataframe"
      ],
      "metadata": {
        "id": "JbriClEV9CW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = red>[write your answer]</font> <br>\n"
      ],
      "metadata": {
        "id": "rrNQ4AtD9RPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2.4** **Drop the rows that have invalid data provided in previous cell** <font color = red> [2 marks]</font> <br>"
      ],
      "metadata": {
        "id": "Qtqtij2-CD2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the irrelevant recipe data\n"
      ],
      "metadata": {
        "id": "jaiy1pYWCFPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2.5** **Update the input_length & pos_length in dataframe**<font color = red> [2 marks]</font> <br>"
      ],
      "metadata": {
        "id": "_RJEStPSC9PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# update the input and pos length in input_length and pos_length\n"
      ],
      "metadata": {
        "id": "XjJd7gPI5_ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2.6** **Validate the input_length and pos_length by checking unequal rows** <font color = red> [1 marks]</font> <br>"
      ],
      "metadata": {
        "id": "qJdYJ2TEDBzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# validate the input length and pos length as input_length and pos_length\n"
      ],
      "metadata": {
        "id": "fdSsdOPM8aXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3** Train Validation Split (70 train - 30 val) <font color = red>[6 marks]</font> <br>"
      ],
      "metadata": {
        "id": "TwKLW4em-qMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1** *Perform train and validation split ratio* <font color = red>[6 marks]</font> <br>\n",
        "Split the dataset with the help of input_tokens and pos_tokens and make a ratio of 70:30 split for training and validation datasets."
      ],
      "metadata": {
        "id": "z_pJDTVO-71z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **3.1.1** **Split the dataset into train_df and val_df into 70:30 ratio** <font color = red> [1 marks]</font> <br>"
      ],
      "metadata": {
        "id": "l-64gdDiIy9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into training and validation sets\n"
      ],
      "metadata": {
        "id": "W20A_-9E_WOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **3.1.2** **Print the first five rows of train_df and val_df** <font color = red> [1 marks]</font> <br>"
      ],
      "metadata": {
        "id": "PUA05_77JRAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first five rows of train_df\n"
      ],
      "metadata": {
        "id": "pgMZfsbV_XhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first five rows of the val_df\n"
      ],
      "metadata": {
        "id": "Kgtg5WE4_d7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **3.1.3** **Extract the dataset into train_df and val_df into X_train, X_val, y_train and y_val and display their length** <font color = red> [2 marks]</font> <br>\n",
        "\n",
        "Extract X_train, X_val, y_train and y_val by extracting the list of input_tokens and pos_tokens from train_df and val_df and also display their length"
      ],
      "metadata": {
        "id": "7prEiaiqI_VZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the training and validation sets by taking input_tokens and pos_tokens\n"
      ],
      "metadata": {
        "id": "BFVnCD71IHXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate the shape of training and validation samples\n"
      ],
      "metadata": {
        "id": "yQPOVz3J_fiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **3.1.4** **Display the number of unique labels present in y_train** <font color = red> [2 marks]</font> <br>"
      ],
      "metadata": {
        "id": "8uicUYglLeiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the number of unique labels present in y_train\n"
      ],
      "metadata": {
        "id": "hzWtzpdINt6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4** Exploratory Recipe Data Analysis on Training Dataset <font color = red>[16 marks]</font> <br>"
      ],
      "metadata": {
        "id": "QFm46QrB4gmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.1** *Flatten the lists for input_tokens & pos_tokens* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a function **flatten_list** for flattening the structure for input_tokens and pos_tokens. The input parameter passed to this function is a nested list.\n",
        "\n",
        "Initialise the dataset_name with a value ***'Training'***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KUWIp0n_NeH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten the list for nested_list (input_tokens, pos_tokens)\n"
      ],
      "metadata": {
        "id": "WzcY0gPiOe8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise the dataset_name\n",
        "dataset_name = 'Training'"
      ],
      "metadata": {
        "id": "DXRda29gNBH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.2** *Extract and validate the tokens after using the flattening technique* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a function named ***extract_and_validate_tokens*** with parameters dataframe and dataset_name (Training/Validation), validate the length of input_tokens and pos_tokens from dataframe and display first 10 records for both the input_tokens and pos_tokens. Execute this function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eGOqhd8OOr1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a extract_and_validate_tokens with parameters (df, dataset_name)\n",
        "# call the flatten_list and apply it on input_tokens and pos_tokens\n",
        "# validate their length and display first 10 records having input and pos tokens\n"
      ],
      "metadata": {
        "id": "H3GMX83xP7ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the tokens and its pos tags\n"
      ],
      "metadata": {
        "id": "ajdbYMgeLpf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.3** *Categorise tokens into labels (unit, ingredient, quantity)* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a function ***categorize_tokens*** to categorise tokens into ingredients, units and quantities by using extracted tokens in the previous code and return a list of ingredients, units and quantities. Execute this function to get the list.\n",
        "\n"
      ],
      "metadata": {
        "id": "htZVn5wcQSok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a categorize_tokens function and provide the tokens and pos_tags as parameters and create ingredient, unit and quantity list and return it\n",
        "# validate the list that it comprised of these labels, if not return empty arrays\n",
        "\n"
      ],
      "metadata": {
        "id": "7xq0a4L7Quct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  call the function to categorise the labels into respective list\n"
      ],
      "metadata": {
        "id": "evcsigvUL7bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.4** *Top 10 Most Frequent Items* <font color = red>[3 marks]</font> <br>\n",
        "\n",
        "Define a function ***get_top_frequent_items*** to display top 10 most frequent items\n",
        "\n",
        "Here, item_list is used as a general parameter where you will call this function for ingredient and unit list\n",
        "\n",
        "Execute this function separately for top 10 most units and ingredients\n",
        "\n"
      ],
      "metadata": {
        "id": "QSGau4EgZCix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function get_top_frequent_items to get the top frequent items by using item_list, pos label and dataset_name(Training/Validation) and return top items\n"
      ],
      "metadata": {
        "id": "kXc8h3H4ZOZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the top ingredients which are frequently seen in the recipe\n"
      ],
      "metadata": {
        "id": "W2jZCCf2MEke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the top units which are frequently seen in the recipe\n"
      ],
      "metadata": {
        "id": "wipghGXAMYQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.5** *Plot Top 10 most frequent items* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9hldpjOHaPVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function ***plot_top_items*** to plot a bar graph on top 10 most frequent items for units and ingredients\n",
        "\n",
        "Here, item_list is used as a general parameter where you will call this function for ingredient and unit list"
      ],
      "metadata": {
        "id": "9ImpWstybDP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define plot top items with parameters - top_item list, label to suggest whether its ingredient or unit, dataset_name\n"
      ],
      "metadata": {
        "id": "Gmsq0L1vaxfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.6** *Perform EDA analysis* <font color = red>[5 marks]</font> <br>\n",
        "\n",
        "Plot the bar plots for ingredients and units and provide the insights for training dataset\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rHusCfkJ4suh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the top frequent ingredients in training data\n"
      ],
      "metadata": {
        "id": "8seIqFKyYFmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the top frequent units in training data\n"
      ],
      "metadata": {
        "id": "fbXAwiUkMtqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5** Exploratory Recipe Data Analysis on Validation Dataset (Optional)<font color = red> [0 marks]</font> <br>"
      ],
      "metadata": {
        "id": "vYh7zbJpCajJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.1** *Execute EDA on Validation Dataset with insights (Optional)* <font color = red> [0 marks]</font> <br>\n",
        "Initialise the dataset_name as ***Validation*** and call the ***plot_top_items*** for top 10 ingredients and units in the recipe data\n",
        "Provide the insights for the same.\n",
        "\n"
      ],
      "metadata": {
        "id": "K2wPIaOGCmk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise the dataset_name\n"
      ],
      "metadata": {
        "id": "atSk0ChLPXHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use extract and validate tokens, categorise tokens, get top frequent items for ingredient list and unit list on validation dataframe\n"
      ],
      "metadata": {
        "id": "AFPxheIuj1o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the top frequent ingredients in validation data\n"
      ],
      "metadata": {
        "id": "ikwox7ccMaU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the top frequent units in training data\n"
      ],
      "metadata": {
        "id": "9QjVeMWpPwKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6** Feature Extraction For CRF Model <font color = red>[30 marks]</font> <br>"
      ],
      "metadata": {
        "id": "IvE92ait9GIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.1** *Define a feature functions to take each token from recipe* <font color = red>[10 marks]</font>\n",
        "\n",
        "Define a function as ***word2features*** which takes a particular recipe and its index to work with all recipe input tokens and include custom key-value pairs.\n",
        "\n",
        "Also, use feature key-value pairs to mark the beginning and end of the sequence and to also check whether the word belongs to unit, quantity etc. Use keyword sets for unit and quantity for differentiating feature functions well. Also make use of relevant regex patterns on fractions, whole numbers etc."
      ],
      "metadata": {
        "id": "Gc5Q_Lj09GIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **6.1.1** **Define keywords for unit and quantity and create a quantity pattern to work on fractions, numbers and decimals** <font color = red>[3 marks]</font> <br>\n",
        "\n",
        "Create sets for **unit_keywords** and ***quantity_keywords*** and include all the words relevant for measuring the ingredients such as cup, tbsp, tsp etc. and in quantity keywords, include words such as half, quarter etc.\n",
        "\n",
        "Also suggested to use regex pattern as ***quantity_pattern*** to work with quantity in any format such as fractions, numbers and decimals.\n",
        "\n",
        "Then, load the spacy model and process the entire sentence"
      ],
      "metadata": {
        "id": "pyxmQ0PrhBra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define unit and quantity keywords along with quantity pattern\n"
      ],
      "metadata": {
        "id": "GhFUPxeth0KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load spaCy model\n"
      ],
      "metadata": {
        "id": "9qmM8rw4VtJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **6.1.2** **Define feature functions for CRF** <font color = red>[7 marks]</font> <br>\n",
        "\n",
        "Define ***word2features*** function and use the parameters such as sentence and its indexing as ***sent*** and ***i*** for extracting token level features for CRF Training.\n",
        "Build ***features*** dictionary, also mark the beginning and end of the sequence and use the ***unit_keywords***, ***quantity_keywords*** and ***quantity_pattern*** for knowing the presence of quantity or unit in the tokens"
      ],
      "metadata": {
        "id": "vrYD5tMNiFc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While building ***features*** dictionary, include\n",
        "- ***Core Features*** - The core features of a token should capture its lexical\n",
        "and grammatical properties. Include attributes like the raw token, its lemma, part-of-speech tag, dependency relation, and shape, as well as indicators for whether it's a stop word, digit, or punctuation. The details of the features are given below:\n",
        "\n",
        "    - `bias` - Constant feature with a fixed value of 1.0 to aid model learning.\n",
        "    - `token` - The lowercase form of the current token.\n",
        "    - `lemma` - The lowercase lemma (base form) of the token.\n",
        "    - `pos_tag` - Part-of-speech (POS) tag of the token.\n",
        "    - `tag` - Detailed POS tag of the token.\n",
        "    - `dep` - Dependency relation of the token in the sentence.\n",
        "    - `shape` - Shape of the token (e.g., \"Xxx\" for \"Milk\").\n",
        "    - `is_stop` - Boolean indicating if the token is a stopword.\n",
        "    - `is_digit` - Boolean indicating if the token consists of only digits.\n",
        "    - `has_digit` - Boolean indicating if the token contains at least one digit.\n",
        "    - `has_alpha` - Boolean indicating if the token contains at least one alphabetic character.\n",
        "    - `hyphenated` - Boolean indicating if the token contains a hyphen (-).\n",
        "    - `slash_present` - Boolean indicating if the token contains a slash (/).\n",
        "    - `is_title` - Boolean indicating if the token starts with an uppercase letter.\n",
        "    - `is_upper` - Boolean indicating if the token is fully uppercase.\n",
        "    - `is_punct` - Boolean indicating if the token is a punctuation mark.\n",
        "\n",
        "- ***Improved Quantity and Unit Detection*** - Use key-value pairs to mark the presence of quantities and units in the features dictionary. Utilise the unit_keywords, quantity_keywords, and quantity_pattern to identify and flag these elements. The details of the features are given below:\n",
        "\n",
        "    - `is_quantity` - Boolean indicating if the token matches a quantity pattern or keyword.\n",
        "    - `is_unit` - Boolean indicating if the token is a known measurement unit.\n",
        "    - `is_numeric` - Boolean indicating if the token matches a numeric pattern.\n",
        "    - `is_fraction` - Boolean indicating if the token represents a fraction (e.g., 1/2).\n",
        "    - `is_decimal` - Boolean indicating if the token represents a decimal number (e.g., 3.14).\n",
        "    - `preceding_word` - The previous token in the sentence, if available.\n",
        "    - `following_word` - The next token in the sentence, if available.\n",
        "\n",
        "- ***Contextual Features*** - Incorporate contextual information by adding features for the preceding and following tokens. Include indicators like BOS and EOS to mark the beginning and end of the sequence, and utilise unit_keywords, quantity_keywords, and quantity_pattern to identify the types of neighboring tokens. The features are given below:\n",
        "\n",
        "    - `prev_token` - The lowercase form of the previous token.\n",
        "    - `prev_is_quantity` - Boolean indicating if the previous token is a quantity.\n",
        "    - `prev_is_digit` - Boolean indicating if the previous token is a digit.\n",
        "    - `BOS` - Boolean indicating if the token is at the beginning of the sentence.\n",
        "    - `next_token` - The lowercase form of the next token.\n",
        "    - `next_is_unit` - Boolean indicating if the next token is a unit.\n",
        "    - `next_is_ingredient` - Boolean indicating if the next token is not a unit or quantity.\n",
        "    - `EOS` - Boolean indicating if the token is at the end of the sentence.\n",
        "\n"
      ],
      "metadata": {
        "id": "yAjf6j-dQtpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define word2features for processing each token in the sentence sent by using index i.\n",
        "# use your own feature functions\n",
        "\n",
        "    # Process the entire sentence with spaCy\n",
        "\n",
        "    # --- Core Features ---\n",
        "\n",
        "    # --- Improved Quantity & Unit Detection ---\n",
        "\n",
        "    # --- Contextual Features ---\n"
      ],
      "metadata": {
        "id": "pRU7efTF9GIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.2** *Preparation of Recipe level features* <font color = red>[2 marks]</font>\n"
      ],
      "metadata": {
        "id": "hJm2nUw0998s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **6.2.1** **Define function to work on all the recipes and call word2features for each recipe** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define ***sent2features*** function and inputs ***sent*** as a parameter and correctly generate feature functions for each token present in the sentence"
      ],
      "metadata": {
        "id": "KL19ooQejA5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define sent2features by working on each token in the sentence and correctly generate dictionaries for features\n"
      ],
      "metadata": {
        "id": "NlQEifz-9GIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.3** *Convert X_train, X_val, y_train and y_val into train and validation feature sets and labels* <font color = red>[6 marks]</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "FOK0t3c6-RiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **6.3.1** **Convert recipe into feature functions by using X_train and X_val** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***X_train_features*** and ***X_val_features*** as list to include the feature functions for each recipe present in training and validation sets"
      ],
      "metadata": {
        "id": "7tsd50b_nX0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert input sentences into feature sets by taking training and validation dataset as X_train_features and X_val_features\n"
      ],
      "metadata": {
        "id": "-bVPGPa39GIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **6.3.2** **Convert lables of y_train and y_val into list** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***y_train_labels*** and ***y_val_labels*** by using the list of y_train and y_val"
      ],
      "metadata": {
        "id": "jcwmwXn-n6cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels into list as y_train_labels and y_val_labels\n"
      ],
      "metadata": {
        "id": "TiGgP3O6nfPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **6.3.3** **Print the length of val and train features and labels** <font color = red>[2 marks]</font> <br>\n",
        "\n"
      ],
      "metadata": {
        "id": "4c-kjqtaoZvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the length of train features and labels\n"
      ],
      "metadata": {
        "id": "mWId2Nn0okMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the length of validation features and labels\n"
      ],
      "metadata": {
        "id": "LAt_m_LubRvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.4** *Applying weights to feature sets* <font color = red>[12 marks]</font> <br>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dZffFBH-pVhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **6.4.1** **Flatten the labels of y_train** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***y_train_flat*** to flatten the structure of nested y_train"
      ],
      "metadata": {
        "id": "Goh_fX-6pqhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten labels in y_train\n"
      ],
      "metadata": {
        "id": "adLWfYn_p3gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **6.4.2** **Count the labels present in training target dataset** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***label_counts*** to count the frequencies of labels present in y_train_flat and retrieve the total samples by using the values of label_counts as ***total_samples***"
      ],
      "metadata": {
        "id": "qk9UMBrbp9dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count label frequencies as label_counts and total_samples as getting the summation of values of label_counts\n"
      ],
      "metadata": {
        "id": "6Kiu8jckqZSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **6.4.3** **Compute weight_dict by using inverse frequency method for label weights** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "- Create ***weight_dict*** as dictionary with label and its inverse frequency count in ***label_counts***\n",
        "\n",
        "- Penalise ingredient label in the dictionary"
      ],
      "metadata": {
        "id": "4aCmDsZYqYA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class weights (inverse frequency method) by considering total_samples and label_counts\n"
      ],
      "metadata": {
        "id": "FpbEAZ3zqxEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# penalise ingredient label\n"
      ],
      "metadata": {
        "id": "hns3HbujXESs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **6.4.4** **Extract features along with class weights** <font color = red>[4 marks]</font> <br>\n",
        "\n",
        "Define a function ***extract_features_with_class_weights*** to work with training and validation datasets and extract features by applying class weights\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V8TdHMlPrhh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply weights to feature extraction in extract_features_with_class_weights by using parameters such as X (input tokens), y(labels) and weight_dict (Class weights)\n"
      ],
      "metadata": {
        "id": "1km6GR4TjXPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **6.4.5** **Execute extract_features_with_class_weights on training and validation datasets** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***X_train_weighted_features*** and ***X_val_weighted_features*** for extracting training and validation features along with their weights by calling ***extract_features_with_class_weights*** on the datasets"
      ],
      "metadata": {
        "id": "51ABmKwKsaiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply manually computed class weights\n"
      ],
      "metadata": {
        "id": "-XUFFnm5sYE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7** Model Building and Training <font color = red>[10 marks]</font> <br>"
      ],
      "metadata": {
        "id": "Aah9bFDlAuzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.1** *Initialise the CRF model and train it* <font color = red>[5 marks]</font>\n",
        "Train the CRF model with the specified hyperparameters such as\n",
        "\n",
        "### CRF Model Hyperparameters Explanation\n",
        "\n",
        "| Parameter                  | Description |\n",
        "|----------------------------|-------------|\n",
        "| **algorithm='lbfgs'**      | Optimisation algorithm used for training. `lbfgs` (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton optimisation method. |\n",
        "| **c1=0.5**                | L1 regularisation term to control sparsity in feature weights. Helps in feature selection. |\n",
        "| **c2=1.0**                | L2 regularisation term to prevent overfitting by penalising large weights. |\n",
        "| **max_iterations=100**     | Maximum number of iterations for model training. Higher values allow more convergence but increase computation time. |\n",
        "| **all_possible_transitions=True** | Ensures that all possible state transitions are considered in training, making the model more robust. |\n",
        "\n",
        "Use weight_dict for training CRF\n",
        "\n"
      ],
      "metadata": {
        "id": "axrvWR9TAuzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise CRF model with the specified hyperparameters and use weight_dict\n",
        "\n",
        "# train the CRF model with the weighted training data\n"
      ],
      "metadata": {
        "id": "jig2J_n1AuzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.2** *Evaluation of Training Dataset using CRF model* <font color = red>[4 marks]</font>\n",
        "Evaluate on training dataset using CRF by using flat classification report and confusion matrix"
      ],
      "metadata": {
        "id": "sDLwvYqOF6m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on the training dataset\n"
      ],
      "metadata": {
        "id": "Us57jWSQ6laL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the flat classification report by using training data for evaluation\n"
      ],
      "metadata": {
        "id": "gNGZnd-D6oq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a confusion matrix on training datset\n"
      ],
      "metadata": {
        "id": "GqP9WBvJ63qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.3** *Save the CRF model* <font color = red>[1 marks]</font>\n",
        "Save the CRF model"
      ],
      "metadata": {
        "id": "Yps2-XscGuHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dump the model using joblib as crf_model.pkl\n"
      ],
      "metadata": {
        "id": "iAYDLatcGzEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8** Prediction and Model Evaluation <font color = red>[3 marks]</font> <br>"
      ],
      "metadata": {
        "id": "agM32oUlBo1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8.1** *Predict and Evaluate the CRF model on validation set* <font color = red>[3 marks]</font>\n",
        "Evaluate the metrics for CRF model by using flat classification report and confusion matrix\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b5BYmkTrBo1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the crf model on validation dataset\n"
      ],
      "metadata": {
        "id": "qhH6Sp8tBo1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify flat classification report\n"
      ],
      "metadata": {
        "id": "SMktt_w1kovB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a confusion matrix on validation dataset\n"
      ],
      "metadata": {
        "id": "eI2tUBRRk4jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9** Error Analysis on Validation Data <font color = red>[10 marks]</font> <br>\n",
        "Investigate misclassified samples in validation dataset and provide the insights\n"
      ],
      "metadata": {
        "id": "8pD6hD3NEV3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9.1** *Investigate misclassified samples in validation dataset* <font color = red>[8 marks]</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "R9tUvjrzFjib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **9.1.1** Flatten the labels of validation data and initialise error data <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "\n",
        "\n",
        "Flatten the true and predicted labels and initialise the error data as ***error_data***"
      ],
      "metadata": {
        "id": "Lb15uObqxKe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten Labels and Initialise Error Data\n"
      ],
      "metadata": {
        "id": "gbgYAjd-UzkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **9.1.2** Iterate the validation data and collect Error Information<font color = red> [2 marks]</font> <br>\n",
        "\n",
        "\n",
        "\n",
        "Iterate through validation data (X_val, y_val_labels, y_pred_val) and compare true vs. predicted labels. Collect error details, including surrounding context, previous/next tokens, and class weights, then store them in error_data"
      ],
      "metadata": {
        "id": "LS9foWfdXHOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate and collect Error Information\n",
        "\n",
        "            # get previous and next tokens with handling for boundary cases\n"
      ],
      "metadata": {
        "id": "_VKLc1s0U0yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **9.1.3** Create dataframe from error_data and print overall accuracy <font color = red>[1 marks]</font> <br>\n",
        "\n",
        "\n",
        "\n",
        "Change error_data into dataframe and then use it to illustrate the overall accuracy of validation data"
      ],
      "metadata": {
        "id": "G_R8CCAFZSzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame and Print Overall Accuracy\n"
      ],
      "metadata": {
        "id": "fUffRP7XU3YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **9.1.4** Analyse errors by label type<font color = red> [3 marks]</font> <br>\n",
        "Analyse errors found in the validation data by each label and display their class weights along with accuracy and also display the error dataframe with token,  previous token, next token, true label, predicted label and context"
      ],
      "metadata": {
        "id": "8OUYHFmgZhgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse errors found in the validation data by each label\n",
        "# and display their class weights along with accuracy\n",
        "# and display the error dataframe with token, previous token, next token, true label, predicted label and context\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zu8CtjU6WR9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9.2** *Provide insights from the validation dataset* <font color = red>[2 marks]</font>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3n74kVvEV3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color = red>[Write your answer]</font>"
      ],
      "metadata": {
        "id": "aWZdf1O_vWnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10** Conclusion (Optional) <font color = red>[0 marks]</font> <br>\n",
        "\n",
        "Write your findings and conclusion."
      ],
      "metadata": {
        "id": "qUjFPBMxH20n"
      }
    }
  ]
}